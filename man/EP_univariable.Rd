% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Uni_EP.R
\name{EP_univariable}
\alias{EP_univariable}
\title{Univariate MEP Bayes with DISCO severity (EP_univariable)}
\usage{
EP_univariable(
  data,
  predictor,
  outcome = "y",
  missing = c("complete", "impute"),
  impute_args = list(),
  n_iter = 20000,
  burn_in = 5000,
  step_hi = 0.3,
  step_lo = 0.12,
  ci_level = 0.95,
  compare = TRUE,
  return_draws = FALSE,
  seed = NULL,
  transform_beta1 = c("none", "logit", "SAS", "Long"),
  sigma0 = 10,
  sigma1_hi = 5,
  sigma1_lo = 0.15,
  kappa_min = 1,
  kappa_max = 2.5,
  tune_threshold_hi = 0.45,
  tune_threshold_lo = 0.2,
  tune_interval = 500
)
}
\arguments{
\item{data}{A data.frame containing \code{outcome} and \code{predictor}.}

\item{predictor}{String; name of the predictor column.}

\item{outcome}{String; name of the binary outcome column (default \code{"y"}).}

\item{missing}{How to handle missing data: \code{"complete"} (drop rows with any
NA in \code{predictor} or \code{outcome}) or \code{"impute"} (impute the
predictor only; outcome NA is always dropped). Default \code{"complete"}.}

\item{impute_args}{Optional list of imputation settings passed to
\code{DISCO::uni_separation()} (e.g., \code{list(numeric_method="median")}).}

\item{n_iter}{Integer; total MCMC iterations (including burn-in).
Default \code{20000}.}

\item{burn_in}{Integer; burn-in iterations discarded from the front.
Default \code{5000}.}

\item{step_hi, step_lo}{RW–MH proposal s.d. blended by severity as
\code{step = step_hi*(1 - severity) + step_lo*severity}.
Defaults \code{0.30} and \code{0.12}.}

\item{ci_level}{Credible interval level in (0,1). Default \code{0.95}.}

\item{compare}{Logical; if \code{TRUE} (default) fit GLM and Firth logistic
(\pkg{logistf}) on the same rows for reference.}

\item{return_draws}{Logical; if \code{TRUE}, include posterior draws on
standardized and original scales. Default \code{FALSE}.}

\item{seed}{Optional integer RNG seed.}

\item{transform_beta1}{One of \code{"none"}, \code{"logit"}, \code{"SAS"}, \code{"Long"}.
If not \code{"none"}, also reports \code{beta0_orig} and the chosen slope on the
original predictor scale. Default \code{"none"} (standardized-only output).}

\item{sigma0}{Prior sd for intercept (logit scale). Default \code{10}.}

\item{sigma1_hi}{Prior sd for slope under mild separation (\code{severity = 0}).
Default \code{5}.}

\item{sigma1_lo}{Prior sd for slope under severe separation (\code{severity = 1}).
Default \code{0.15}.}

\item{kappa_min, kappa_max}{Exponential-power shapes at \code{severity = 0} and
\code{1}, blended linearly. Defaults \code{1} and \code{2.5}.}

\item{tune_threshold_hi, tune_threshold_lo}{Burn-in acceptance thresholds for
auto step-size tuning (increase if \code{> hi}; decrease if \code{< lo}).
Defaults \code{0.45} and \code{0.20}.}

\item{tune_interval}{Iterations between tuning checks during burn-in.
Default \code{500}.}
}
\value{
A list with components:
\itemize{
\item \code{predictor}, \code{outcome}
\item \code{disco}: list with \code{separation_type}, \code{severity_score},
\code{boundary_threshold}, \code{single_tie_boundary}, \code{missing_info}.
\item \code{prior}: list with \code{mu}, \code{Sigma}, \code{kappa},
\code{sigma0}, \code{sigma1} (slope prior scale implied by severity).
\item \code{mcmc}: list with \code{acceptance_rate}, \code{step_size_used},
\code{n_iter}, \code{burn_in}.
\item \code{posterior}: data.frame of summaries with columns \code{Param},
\code{Mean}, \code{SD}, \code{CI_low}, \code{CI_high}, containing
\code{beta0}, \code{beta1} (standardized) and, if requested via
\code{transform_beta1}, \code{beta0_orig} plus one of
\code{beta1_logit}/\code{beta1_SAS}/\code{beta1_Long}.
\item \code{comparators}: list with \code{glm} and \code{firth} coefficient
vectors when available.
\item \code{rows_used}: integer indices of rows used after missing handling.
\item \code{draws} (optional): list with \code{chain_std} and \code{chain_orig}
(included when \code{return_draws = TRUE}).
}
}
\description{
Runs a DISCO-based univariate separation diagnostic, constructs an adaptive
Multivariate Exponential Power (MEP) prior from the severity score, and fits
a univariate logistic model (intercept + one predictor) via random-walk
Metropolis–Hastings (RW–MH).
}
\details{
\strong{Adaptive prior from severity.}
Let \eqn{\bar{y}} be the sample mean on the analyzed rows and \eqn{s_x} the
predictor SD (on those rows). The MEP prior on \eqn{(\beta_0,\beta_1)} (working,
standardized-\eqn{X} scale) uses
\deqn{\mu = (\mathrm{logit}(\bar{y}),\, 0),\quad
      \Sigma = \mathrm{diag}(\sigma_0^2,\ \sigma_1^2),\quad
      \log \sigma_1 = (1-s)\log\sigma_{1,\mathrm{hi}} + s\log\sigma_{1,\mathrm{lo}},}
with \eqn{s} the severity score from \code{DISCO::uni_separation()} and
\eqn{\kappa = \kappa_{\min} + s(\kappa_{\max} - \kappa_{\min})}.

\strong{Back-transform formulas.} If requested via \code{transform_beta1}:
\deqn{\beta_{1}^{\mathrm{logit}} = \beta_{1}^{\mathrm{std}}/s_x,\qquad
      \beta_{0}^{\mathrm{orig}} = \beta_{0}^{\mathrm{std}} - \beta_{1}^{\mathrm{std}}\cdot (\bar{x}/s_x),}
and we also report one of:
\deqn{\beta_{1}^{\mathrm{SAS}}  = \beta_{1}^{\mathrm{logit}} \cdot \pi/\sqrt{3},\qquad
      \beta_{1}^{\mathrm{Long}} = \beta_{1}^{\mathrm{logit}} \cdot (\pi/\sqrt{3} + 1).}

\strong{Tuning.} During burn-in, the RW–MH step size is multiplicatively
adapted every \code{tune_interval} iterations to aim for an acceptance rate
between \code{tune_threshold_lo} and \code{tune_threshold_hi}.
}
\section{Output conventions}{

By default, the function returns the \strong{standardized} coefficients:
\itemize{
\item \code{beta0}: intercept in the standardized-\eqn{X} (z-scored) working scale.
\item \code{beta1}: slope with respect to the standardized predictor.
}
Optionally, set \code{transform_beta1} to back-transform to the original
predictor scale, which adds:
\itemize{
\item \code{beta0_orig}: intercept on the original predictor scale.
\item \code{beta1_logit} \emph{or} \code{beta1_SAS} \emph{or} \code{beta1_Long}:
slope per 1 unit change in the original predictor, on the chosen scale.
}
}

\section{Interpretation}{

\itemize{
\item \code{beta1} (default) — change in log-odds per 1 SD increase in the predictor (standardized scale).
\item \code{beta1_logit} — change in log-odds per 1 unit increase in the original predictor.
\item \code{beta1_SAS} and \code{beta1_Long} — alternative effect scalings derived from \code{beta1_logit}.
\item \code{beta0_orig} is the intercept consistent with original predictor units.
}
}

\examples{
\donttest{
## Toy data: y ~ Bernoulli(logit^{-1}(-0.2 + 1.0 * x)) on ORIGINAL x units
set.seed(1)
n  <- 60
x  <- rnorm(n, mean = 0.3, sd = 1)
eta <- -0.2 + 1.0 * x
y  <- rbinom(n, size = 1, prob = stats::plogis(eta))
df <- data.frame(y = y, x = x)

## 1) Default: STANDARDIZED coefficients (beta0, beta1)
fit_std <- EP_univariable(
  data = df, predictor = "x", outcome = "y",
  n_iter = 6000, burn_in = 2000, seed = 42  # reduced iters for example speed
)
fit_std$posterior

## 2) Back-transform slope to ORIGINAL-x units on the LOGIT scale
fit_logit <- EP_univariable(
  data = df, predictor = "x", outcome = "y",
  transform_beta1 = "logit",
  n_iter = 6000, burn_in = 2000, seed = 42
)
fit_logit$posterior

## 3) Alternative effect scales on ORIGINAL-x units (choose ONE)
fit_sas <- EP_univariable(
  data = df, predictor = "x", outcome = "y",
  transform_beta1 = "SAS",
  n_iter = 6000, burn_in = 2000, seed = 42
)
fit_long <- EP_univariable(
  data = df, predictor = "x", outcome = "y",
  transform_beta1 = "Long",
  n_iter = 6000, burn_in = 2000, seed = 42
)
fit_sas$posterior   # contains beta1_SAS
fit_long$posterior  # contains beta1_Long
}

}
\seealso{
\code{\link[DISCO]{uni_separation}} for severity diagnostics.
}
