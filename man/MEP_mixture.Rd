% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MEP_mixture.R
\name{MEP_mixture}
\alias{MEP_mixture}
\title{Severity-Adaptive MEP for Mixture (multi-predictor) Logistic}
\usage{
MEP_mixture(
  y,
  X,
  missing = c("complete"),
  severity_missing = c("complete", "impute"),
  impute_args = list(),
  n_iter_grid = 10000,
  burn_in_grid = 1000,
  step_size = 0.4,
  mu_intercept_offsets = seq(-1, 1, by = 0.2),
  sigma0_intercept = 10,
  sigma_global_multipliers = c(0.1, 0.5, 1, 2, 5, 10),
  sigma_hi = 5,
  sigma_lo = 0.15,
  kappa_min = 1,
  kappa_max = 2.5,
  kappa_delta = seq(-0.5, 0.5, by = 0.2),
  accept_window = c(0.3, 0.4),
  accept_target = 0.35,
  ref = NULL,
  compare = TRUE,
  seed = 2025,
  return_draws = FALSE
)
}
\arguments{
\item{y}{Numeric binary vector (0/1; logical or 2-level factor/character accepted; coerced to 0/1).}

\item{X}{Matrix or data.frame of predictors (no intercept). May include factors.
Rows must align with \code{y}.}

\item{missing}{How to treat missing values in \emph{modeling}: only \code{"complete"} is
supported; rows with any NA in \code{y} or \code{X} are dropped. Default \code{"complete"}.}

\item{severity_missing}{How to treat missing values when computing per-predictor DISCO
severities: passed to \code{DISCO::uni_separation()} (\code{"complete"} or \code{"impute"}).
Default \code{"complete"}.}

\item{impute_args}{Optional list of imputation settings used only when
\code{severity_missing = "impute"} (see \code{DISCO::uni_separation}).}

\item{n_iter_grid}{Integer; MH iterations per grid point (including burn-in). Default \code{10000}.}

\item{burn_in_grid}{Integer; burn-in iterations per grid point. Default \code{1000}.}

\item{step_size}{Proposal standard deviation for RW–MH. Default \code{0.40}.}

\item{mu_intercept_offsets}{Numeric vector of offsets added to \eqn{\mathrm{logit}(\bar{y})}
for the intercept prior mean grid. Default \code{seq(-1, 1, by = 0.2)}.}

\item{sigma0_intercept}{Prior \eqn{\sigma_0} (sd) for the intercept (logit scale). Default \code{10}.}

\item{sigma_global_multipliers}{Numeric vector of global multipliers applied to all
slope prior scales (after severity anchoring). Default \code{c(0.1, 0.5, 1, 2, 5, 10)}.}

\item{sigma_hi}{Slope prior sd under mild separation (\eqn{s=0}). Default \code{5}.}

\item{sigma_lo}{Slope prior sd under severe separation (\eqn{s=1}). Default \code{0.15}.}

\item{kappa_min, kappa_max}{EP shape at \eqn{s=0} and \eqn{s=1}. Defaults \code{1}, \code{2.5}.}

\item{kappa_delta}{Offsets added around the anchor-average \eqn{\kappa} to form the grid.
Default \code{seq(-0.5, 0.5, by = 0.2)} (truncated to the interval \eqn{[0.5, 3]}).}

\item{accept_window}{Numeric length-2 vector; acceptable MH acceptance interval.
Default \code{c(0.30, 0.40)}.}

\item{accept_target}{Scalar acceptance target used as a fallback (closest is preferred)
when no grid point falls inside \code{accept_window}. Default \code{0.35}.}

\item{ref}{Predictor \emph{name} or \emph{index} (in the original \code{X}) to serve as
ratio denominator. If \code{NULL} (default), the predictor with the highest
univariate severity is used. If the reference is a factor, the denominator
column is the \emph{first} dummy generated for that factor.}

\item{compare}{Logical; if \code{TRUE} (default), fit GLM and Firth (\code{logistf})
on the standardized encoded design as comparators.}

\item{seed}{For reproducibility, default 2025.}

\item{return_draws}{Logical; if \code{TRUE}, return the post-burn MH chain for the
selected grid run.}
}
\value{
A list with components:
\itemize{
\item \code{ref_predictor}: list with \code{index} (1-based in original \code{X}) and \code{name}.
\item \code{severity}: data.frame with per-\emph{original} predictor severities used to anchor prior scales.
\item \code{grid_summary}: data.frame summarizing all grid runs
(\code{grid_id}, \code{mu}, \code{sigma_diag}, \code{kappa}, \code{acceptance_rate},
\code{prop_matched}, \code{posterior_ratio_std}).
\item \code{best}: list describing the selected grid point
(\code{grid_id}, \code{mu}, \code{sigma_diag}, \code{kappa},
\code{acceptance_rate}, \code{prop_matched}).
\item \code{posterior}: list with
\itemize{
\item \code{means_std}: posterior means on the standardized encoded design
(length = intercept + encoded slopes),
\item \code{effects}: data.frame with per encoded column:
\code{Predictor}, \code{Scaled} (standardized slope),
\code{b_A_original}, \code{b_SAS_original}, \code{b_Long_original}.
}
\item \code{ratios}: list of vectors on the encoded design:
\code{GLM_beta}, \code{GLM_ratio}, \code{Firth_beta}, \code{Firth_ratio},
\code{MEP_beta_std}, \code{MEP_ratio_std},
and back-transform families \code{MEP_beta_b_A_original},
\code{MEP_beta_b_SAS_original}, \code{MEP_beta_b_Long_original},
plus their ratios \code{MEP_ratio_b_A_original},
\code{MEP_ratio_b_SAS_original}, \code{MEP_ratio_b_Long_original}.
Ratios divide by the encoded column corresponding to the reference predictor
(first dummy if factor).
\item \code{comparators}: list with raw \code{glm} and \code{firth} fits (if available).
\item \code{draws}: matrix of MH draws after burn-in for the selected run
(returned only when \code{return_draws = TRUE}).
}
}
\description{
Fits a multi-predictor logistic model with a \strong{Multivariate Exponential Power (MEP)}
prior using a simple Random-Walk Metropolis–Hastings (RW–MH) sampler, where the \emph{slope
prior scales} are \strong{anchored by univariate DISCO severities}. A small grid over the
intercept prior mean, a global multiplier on slope scales, and the EP shape \eqn{\kappa}
is explored; one run is selected via acceptance window, posterior predictive agreement,
and—when available—closeness to GLM coefficient ratios relative to a reference predictor.
}
\details{
\strong{Design encoding.} Predictors in \code{X} are expanded with
\code{model.matrix(~ ., data = X)} (intercept dropped for slopes). Numeric columns remain
one column each. Factor columns are expanded to treatment-contrast dummies (baseline is
the first level). The RW–MH is run on the \emph{standardized} encoded design (z-scored
columns), but standardized back-transforms (A, SAS, Long) are reported for each encoded
column using the \emph{unscaled} encoded design.

\strong{Standardization & back-transforms.} The sampler runs on z-scored encoded columns.
Reported \code{Scaled} slopes are posterior means on this working scale. Back-transforms
use the encoded column SDs: \code{A} is per-SD effect on log-odds;
\code{SAS} is \code{A} multiplied by \eqn{\pi/\sqrt{3}};
\code{Long} is \code{A} multiplied by \eqn{\pi/\sqrt{3} + 1}.

\strong{Reference predictor & ratios.} The ratio denominator is chosen from the original
\code{X} columns (highest DISCO severity by default). If that predictor is a factor,
the denominator is the \emph{first} dummy column generated for that factor by
\code{model.matrix()}. GLM/Firth ratios are computed on the standardized encoded design.

\strong{Missing data.} This version supports only \code{missing = "complete"} for modeling
(drop rows with NA in \code{y} or any column of \code{X}). For severity anchoring you may use
\code{severity_missing = "impute"} with \code{impute_args} which are passed to
\code{DISCO::uni_separation()}.
}
\section{What this function does}{

\itemize{
\item For each \emph{original} predictor in \code{X}, compute a univariate \strong{severity}
using \code{DISCO::uni_separation()} on the modeling rows.
\item Map severity \eqn{s \in [0,1]} to an anchor slope prior scale \eqn{\sigma_j(s)}
and an anchor \eqn{\kappa_j(s)}; the intercept uses a wide prior.
\item Build a small grid: intercept mean offsets, global multipliers on the \eqn{\sigma_j},
and \eqn{\kappa} around the anchor average; run RW–MH for each grid point.
\item Select one run using a score favoring acceptance in a target window, high posterior
predictive agreement, and closeness of standardized
slope ratios to GLM ratios (with a single \emph{reference} denominator).
}
}

\section{Factor handling & column names}{

\itemize{
\item \strong{Numeric predictors} appear as a single column with their original name
(e.g., \code{X3}). No suffixes are added.
\item \strong{Factor predictors} are expanded by \code{model.matrix()} using treatment
contrasts with the \emph{first level as the baseline}. For a two-level factor
\code{X3} with levels \code{A} and \code{B} (baseline = \code{A}), the encoded
design includes a single dummy column \code{X3B}, which equals 1 when
\code{X3 == "B"} and 0 when \code{X3 == "A"}. The coefficient for \code{X3B}
is the log-odds difference \emph{B vs A} (controlling for other predictors).
\item The output \code{posterior$effects$Predictor} uses these encoded names: numeric
predictors as \code{"Xk"}; factor dummies as \code{"FactorLevel"} (e.g., \code{X3B}).
Back-transforms (\code{b_A_original}, \code{b_SAS_original}, \code{b_Long_original})
are computed per encoded column using that column’s standard deviation
(for a 0/1 dummy with prevalence \eqn{p}, \eqn{\mathrm{sd}=\sqrt{p(1-p)}}).
\item \strong{Change the baseline} beforehand to alter dummy labels:
\preformatted{X$X3 <- stats::relevel(X$X3, ref = "B")  # baseline becomes B; dummy shows as X3A}
\item If you truly intend to \emph{treat a factor as numeric}, convert it yourself:
\preformatted{X$X3 <- as.numeric(X$X3)  # no dummy expansion; column remains 'X3'}
}
}

\examples{
\donttest{

## Example 2: toy with uni- and latent separation
## X3 alone perfectly separates y (A for 0s, B for 1s);
## X1 and X2 do not separate alone, but z = X1 - 5*X2 does (latent separation).
set.seed(1)
y <- c(0,0,0,0, 1,1,1,1)
X_toy <- data.frame(
  X1 = c(-1.86, -0.81,  1.32, -0.40,  0.91,  2.49,  0.34,  0.25),
  X2 = c( 0.52,  -0.07,  0.60,  0.67, -1.39,  0.16, -1.40, -0.09),
  X3 = factor(c(rep("A", 4), rep("B", 4)))
)
# Univariate diagnostics
d3 <- DISCO::uni_separation(data.frame(y=y, X3=X_toy$X3), "X3", "y", "complete")
d1 <- DISCO::uni_separation(data.frame(y=y, X1=X_toy$X1), "X1", "y", "complete")
d2 <- DISCO::uni_separation(data.frame(y=y, X2=X_toy$X2), "X2", "y", "complete")
d3$separation_type; d1$separation_type; d2$separation_type

# Latent (manual): z = X1 - 5*X2 separates perfectly
z <- X_toy$X1 - 5 * X_toy$X2
dz <- DISCO::uni_separation(data.frame(y = y, z = z), "z", "y", "complete")
dz$separation_type  # "Perfect separation"

# Latent (automatic): inclusion-minimal separating subsets of {X1, X2}
lat <- DISCO::latent_separation(
  y = y,
  X = X_toy[, c("X1", "X2")],
  find_minimal  = TRUE,
  mode          = "either",
  missing       = "complete",
  scale_X       = FALSE
)
names(lat$minimal_subsets)              # e.g., "X1_X2"
lat$minimal_subsets[[1]]$type           # "Perfect separation"
lat$minimal_subsets[[1]]$vars           # c("X1","X2")

# Fit MEP_mixture (factor X3 will appear as a dummy 'X3B' = B vs A)
fit_toy <- MEP_mixture(y, X_toy, n_iter_grid = 4000, burn_in_grid = 1000, seed = 9)
fit_toy$ref_predictor
fit_toy$posterior$effects     # includes "X3B" (dummy for B vs A)

# Change baseline level to rename the dummy (now "X3A" = A vs B)
X_toy2 <- X_toy
X_toy2$X3 <- stats::relevel(X_toy2$X3, ref = "B")
fit_toy2 <- MEP_mixture(y, X_toy2, n_iter_grid = 2000, burn_in_grid = 500, seed = 9)
fit_toy2$posterior$effects
}

}
\seealso{
\code{\link[DISCO]{uni_separation}}, \code{\link[logistf]{logistf}}
}
