% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MEP_mixture.R
\name{MEP_mixture}
\alias{MEP_mixture}
\title{Severity-Adaptive MEP for Mixture (multi-predictor) Logistic}
\usage{
MEP_mixture(
  y,
  X,
  missing = c("complete", "impute"),
  impute_args = list(),
  n_iter_grid = 10000,
  burn_in_grid = 1000,
  init_beta = 0.01,
  step_size = 0.4,
  mu_intercept_offsets = seq(-1, 1, by = 0.2),
  sigma0_intercept = 10,
  sigma_global_multipliers = c(0.1, 0.5, 1, 2, 5, 10),
  sigma_hi = 5,
  sigma_lo = 0.15,
  kappa_min = 1,
  kappa_max = 2.5,
  kappa_delta = seq(-0.5, 0.5, by = 0.2),
  accept_window = c(0.3, 0.4),
  accept_target = 0.35,
  ref = NULL,
  transform_back = c("none", "logit", "SAS", "Long"),
  ci_level = 0.95,
  seed = NULL,
  return_draws = FALSE,
  ess_threshold = 150,
  geweke_z_threshold = 2
)
}
\arguments{
\item{y}{Numeric binary vector (0/1; logical or 2-level factor/character accepted; coerced to 0/1).}

\item{X}{Matrix or data.frame of predictors (no intercept). May include factors.
Rows must align with \code{y}.}

\item{missing}{One of \code{"complete"} or \code{"impute"}; the same choice is applied
to both modeling and severity diagnostics (shared rows/values). Default \code{"complete"}.}

\item{impute_args}{Optional list controlling simple external imputation when
\code{missing = "impute"}. Supported:
\code{numeric_method = "median"|"mean"} (default \code{"median"}),
\code{factor_method = "mode"} (default \code{"mode"}).}

\item{n_iter_grid}{Integer; MH iterations per grid point (including burn-in). Default \code{10000}.}

\item{burn_in_grid}{Integer; burn-in iterations per grid point. Default \code{1000}.}

\item{init_beta}{Initial value(s) for the MH chain. Default \code{0.01}.}

\item{step_size}{Proposal standard deviation for RW–MH. Default \code{0.40}.}

\item{mu_intercept_offsets}{Numeric vector of offsets added to \eqn{\mathrm{logit}(\bar{y})}
for the intercept prior mean grid. Default \code{seq(-1, 1, by = 0.2)}.}

\item{sigma0_intercept}{Prior \eqn{\sigma_0} (sd) for the intercept (logit scale). Default \code{10}.}

\item{sigma_global_multipliers}{Numeric vector of global multipliers applied to all
slope prior scales (after severity anchoring). Default \code{c(0.1, 0.5, 1, 2, 5, 10)}.}

\item{sigma_hi}{Slope prior sd under mild separation (\eqn{s=0}). Default \code{5}.}

\item{sigma_lo}{Slope prior sd under severe separation (\eqn{s=1}). Default \code{0.15}.}

\item{kappa_min, kappa_max}{EP shape at \eqn{s=0} and \eqn{s=1}. Defaults \code{1}, \code{2.5}.}

\item{kappa_delta}{Offsets added around the anchor-average \eqn{\kappa} to form the grid.
Default \code{seq(-0.5, 0.5, by = 0.2)} (truncated to the interval \eqn{[0.5, 3]}).}

\item{accept_window}{Numeric length-2 vector; acceptable MH acceptance interval.
Default \code{c(0.30, 0.40)}.}

\item{accept_target}{Scalar acceptance target used as a fallback (closest is preferred)
when no grid point falls inside \code{accept_window}. Default \code{0.35}.}

\item{ref}{Predictor \emph{name} or \emph{index} (in the original \code{X}) to serve as
ratio denominator. If \code{NULL} (default), the predictor with the highest
univariate severity is used. If the reference is a factor, the denominator
column is the \emph{first} dummy generated for that factor.}

\item{transform_back}{One of \code{"none"}, \code{"logit"}, \code{"SAS"}, \code{"Long"}. Controls which
back-transform is reported alongside standardized coefficients. Default \code{"none"}.
\itemize{
\item \code{"logit"}: per-\strong{unit} effect on the original encoded column,
computed as \eqn{\beta^{\mathrm{std}}/s_x}.
\item \code{"SAS"}: \eqn{\beta^{\mathrm{SAS}} = \beta^{\mathrm{logit}} \cdot \pi/\sqrt{3}}.
\item \code{"Long"}: \eqn{\beta^{\mathrm{Long}} = \beta^{\mathrm{logit}} \cdot (\pi/\sqrt{3} + 1)}.
}}

\item{ci_level}{Credible interval level in (0,1). Default \code{0.95}.}

\item{seed}{Optional integer; if provided, sets RNG seed for reproducibility.}

\item{return_draws}{Logical; if \code{TRUE}, return the post-burn MH chain for the
selected grid run.}

\item{ess_threshold}{Numeric; minimum effective sample size (ESS) required (across all
parameters) to declare convergence when \code{coda} is available. Default \code{150}.}

\item{geweke_z_threshold}{Numeric; maximum allowed absolute Geweke z-score (across all
parameters) to declare convergence when \code{coda} is available. Default \code{2}.}
}
\value{
A list with components:
\itemize{
\item \code{ref_predictor}: list with \code{index} (1-based in original \code{X}) and \code{name}.
\item \code{severity}: data.frame with per-\emph{original} predictor severities used to anchor prior scales.
\item \code{grid_summary}: data.frame summarizing all grid runs
(\code{grid_id}, \code{mu}, \code{sigma_diag}, \code{kappa}, \code{acceptance_rate},
\code{prop_matched}, \code{posterior_ratio_std}).
\item \code{best}: list describing the selected grid point
(\code{grid_id}, \code{mu}, \code{sigma_diag}, \code{kappa},
\code{acceptance_rate}, \code{prop_matched}).
\item \code{posterior}: list with
\itemize{
\item \code{means_std}: posterior means on the standardized encoded design
(length = intercept + encoded slopes),
\item \code{effects}: data.frame with per encoded column:
\code{Predictor}, \code{Scaled}, \code{Scaled_CI_low}, \code{Scaled_CI_high},
plus \strong{one optional trio} depending on \code{transform_back}:
\code{b_logit_original}/\code{b_logit_CI_low}/\code{b_logit_CI_high} \emph{or}
\code{b_SAS_original}/\code{b_SAS_CI_low}/\code{b_SAS_CI_high} \emph{or}
\code{b_Long_original}/\code{b_Long_CI_low}/\code{b_Long_CI_high}.
}
\item \code{draws}: matrix of MH draws after burn-in for the selected run
(returned only when \code{return_draws = TRUE}).
}
}
\description{
Fits a multi-predictor logistic model with a \strong{Multivariate Exponential Power (MEP)}
prior using a simple Random-Walk Metropolis–Hastings (RW–MH) sampler, where the \emph{slope
prior scales} are \strong{anchored by univariate DISCO severities}. A small grid over the
intercept prior mean, a global multiplier on slope scales, and the EP shape \eqn{\kappa}
is explored; one run is selected via acceptance window, posterior predictive agreement,
and—when available—closeness to GLM coefficient ratios relative to a reference predictor.
}
\details{
\strong{Design encoding.} Predictors in \code{X} are expanded with
\code{model.matrix(~ ., data = X)} (intercept dropped for slopes). Numeric columns remain
one column each. Factor columns are expanded to treatment-contrast dummies (baseline is
the first level). The RW–MH is run on the \emph{standardized} encoded design (z-scored
columns). Optionally, coefficients can be back-transformed to the original (unscaled)
encoded columns via \code{transform_back = "logit"|"SAS"|"Long"}.

\strong{Standardization & back-transforms.} The sampler runs on z-scored encoded columns.
\code{Scaled} slopes are posterior means on this working scale. Let \eqn{s_x} be the SD of the
unscaled encoded column: \code{logit} back-transform uses \eqn{\beta^{\mathrm{std}}/s_x};
\code{SAS} and \code{Long} multiply the \code{logit} transform by \eqn{\pi/\sqrt{3}} and \eqn{\pi/\sqrt{3}+1}
respectively.

\strong{Reference predictor & ratios.} The ratio denominator is chosen from the original
\code{X} columns (highest DISCO severity by default). If that predictor is a factor,
the denominator is the \emph{first} dummy column generated for that factor by
\code{model.matrix()}. GLM ratios are computed on the standardized encoded design.

\strong{Shared missing handling.} The \code{missing} choice governs a single preprocessing
step applied to \verb{(y, X)}. With \code{missing = "complete"} we drop rows with any NA in
\code{y} or \code{X}. With \code{missing = "impute"} we drop rows with NA in \code{y} and impute
NAs in \code{X} using \code{impute_args}. The same processed data are then used for both the
MEP fit and the DISCO severities (we call \code{DISCO::uni_separation(..., missing = "complete")}
because the data are already prepared). \strong{Numeric predictors are z-scored when computing
DISCO severities} (factors unchanged) to align the anchoring scale with the modeling scale.
}
\section{What this function does}{

\itemize{
\item Applies a single missing-data policy to \verb{(y, X)} \strong{once} (complete-case or external imputation).
\item For each \emph{original} predictor in \code{X}, computes a univariate \strong{severity}
using \code{DISCO::uni_separation()} on the same rows used for modeling.
Numeric predictors are \strong{z-scored for the severity computation} (factors unchanged).
\item Maps severity \eqn{s \in [0,1]} to an anchor slope prior scale \eqn{\sigma_j(s)}
and an anchor \eqn{\kappa_j(s)}; the intercept uses a wide prior.
\item Builds a small grid: intercept mean offsets, global multipliers on the \eqn{\sigma_j},
and \eqn{\kappa} around the anchor average; runs RW–MH for each grid point.
\item Selects one run using a score favoring acceptance in a target window, high posterior
predictive agreement, and closeness of standardized slope ratios to \strong{GLM} ratios
(with a single \emph{reference} denominator).
}
}

\section{Factor handling & column names}{

\itemize{
\item \strong{Numeric predictors} appear as a single column with their original name
(e.g., \code{X3}). No suffixes are added.
\item \strong{Factor predictors} are expanded by \code{model.matrix()} using treatment
contrasts with the \emph{first level as the baseline}. For a two-level factor
\code{X3} with levels \code{A} and \code{B} (baseline = \code{A}), the encoded
design includes a single dummy column \code{X3B}, which equals 1 when
\code{X3 == "B"} and 0 when \code{X3 == "A"}. The coefficient for \code{X3B}
is the log-odds difference \emph{B vs A} (controlling for other predictors).
\item The output \code{posterior$effects$Predictor} uses these encoded names: numeric
predictors as \code{"Xk"}; factor dummies as \code{"FactorLevel"} (e.g., \code{X3B}).
\item \strong{Change the baseline} beforehand to alter dummy labels:
\preformatted{X$X3 <- stats::relevel(X$X3, ref = "B")  # baseline becomes B; dummy shows as X3A}
\item If you truly intend to \emph{treat a factor as numeric}, convert it yourself:
\preformatted{X$X3 <- as.numeric(X$X3)  # no dummy expansion; column remains 'X3'}
}
}

\examples{
\donttest{
## Toy data with one numeric factor and one 2-level factor
y <- c(0,0,0,0, 1,1,1,1)
X_toy <- data.frame(
  X1 = c(-1.86, -0.81,  1.32, -0.40,  0.91,  2.49,  0.34,  0.25),
  X2 = c( 0.52,  -0.07,  0.60,  0.67, -1.39,  0.16, -1.40, -0.09),
  X3 = factor(c(rep("A", 4), rep("B", 4)))
)

## 0) Univariate DISCO diagnostics (complete-case)
d3 <- DISCO::uni_separation(data.frame(y=y, X3=X_toy$X3), "X3", "y", "complete")
d1 <- DISCO::uni_separation(data.frame(y=y, X1=X_toy$X1), "X1", "y", "complete")
d2 <- DISCO::uni_separation(data.frame(y=y, X2=X_toy$X2), "X2", "y", "complete")
d3$separation_type; d1$separation_type; d2$separation_type

## 1) Standardized-only coefficients (includes CIs for Scaled slopes)
fit_std <- MEP_mixture(
  y, X_toy,
  n_iter_grid = 4000, burn_in_grid = 1000, seed = 42,
  transform_back = "none", ci_level = 0.95
)
head(fit_std$posterior$effects)
# Columns: Predictor, Scaled, Scaled_CI_low, Scaled_CI_high

## 2) Back-transform to LOGIT (per-encoded-unit effect) with CIs
fit_logit <- MEP_mixture(
  y, X_toy,
  n_iter_grid = 4000, burn_in_grid = 1000, seed = 42,
  transform_back = "logit"
)
subset(fit_logit$posterior$effects,
       select = c("Predictor","b_logit_original","b_logit_CI_low","b_logit_CI_high"))

## 3) Alternative effect scales with CIs (choose ONE per run)
fit_sas <- MEP_mixture(
  y, X_toy,
  n_iter_grid = 4000, burn_in_grid = 1000, seed = 42,
  transform_back = "SAS"
)
fit_long <- MEP_mixture(
  y, X_toy,
  n_iter_grid = 4000, burn_in_grid = 1000, seed = 42,
  transform_back = "Long"
)
subset(fit_sas$posterior$effects,
       select = c("Predictor","b_SAS_original","b_SAS_CI_low","b_SAS_CI_high"))
subset(fit_long$posterior$effects,
       select = c("Predictor","b_Long_original","b_Long_CI_low","b_Long_CI_high"))

## 4) Change baseline level to rename the dummy (now "X3A" = A vs B)
X_toy2 <- X_toy
X_toy2$X3 <- stats::relevel(X_toy2$X3, ref = "B")
fit_base <- MEP_mixture(
  y, X_toy2,
  n_iter_grid = 3000, burn_in_grid = 800, seed = 7,
  transform_back = "logit"
)
head(fit_base$posterior$effects)   # dummy appears as "X3A"

## 5) Shared missing handling
X_miss <- X_toy
X_miss$X1[c(2,6)] <- NA      # numeric NA
X_miss$X3[7]      <- NA      # factor NA

# (a) complete-case: drops rows with any NA in X
fit_cc <- MEP_mixture(
  y, X_miss,
  missing = "complete",
  n_iter_grid = 3000, burn_in_grid = 800, seed = 9, transform_back = "Long"
)
head(fit_cc$posterior$effects)

# (b) impute: imputes X (numeric=median; factor=mode by default),
#     then uses the same imputed data for DISCO + model
fit_im <- MEP_mixture(
  y, X_miss,
  missing = "impute",
  impute_args = list(numeric_method = "median"),
  n_iter_grid = 3000, burn_in_grid = 800, seed = 9, transform_back = "logit"
)
head(fit_im$posterior$effects)

## 6) Inspect selection & ratios
fit <- fit_logit
fit$severity
head(fit$grid_summary)
fit$best
}
}
\seealso{
\code{DISCO::uni_separation}, \code{DISCO::latent_separation}
}
